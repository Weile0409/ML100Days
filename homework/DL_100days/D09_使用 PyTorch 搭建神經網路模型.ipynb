{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搭建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearBNAC(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bias=True, dropout=0.3, is_output=False):\n",
    "        super(LinearBNAC, self).__init__()\n",
    "        if is_output and out_channels==1:\n",
    "            self.linear = nn.Sequential(\n",
    "                nn.Linear(in_channels, out_channels, bias=bias),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        elif is_output:\n",
    "            self.linear = nn.Sequential(\n",
    "                nn.Linear(in_channels, out_channels, bias=bias),\n",
    "                nn.Softmax(dim=1)\n",
    "            )   \n",
    "        else:\n",
    "            self.linear = nn.Sequential(\n",
    "                nn.Linear(in_channels, out_channels, bias=bias),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                nn.LeakyReLU(inplace=True)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out=self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dimention, output_classes=1):\n",
    "        super(Model, self).__init__()\n",
    "        self.layer1 = LinearBNAC(input_dimention, 128)\n",
    "        self.layer2 = LinearBNAC(128,64)\n",
    "        self.layer3 = LinearBNAC(64,32)\n",
    "        self.output = LinearBNAC(32, output_classes, is_output=True)\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.output(x)\n",
    "        return x \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 準備輸入資料、優化器、標籤資料、模型輸出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_dimention=256,output_classes=10)\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=1e-3, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "input_features = 256\n",
    "dummy_input = torch.randn(batch_size, input_features,)\n",
    "\n",
    "#target = torch.empty(4, dtype=torch.float).random_(10)\n",
    "target = torch.tensor([9., 5., 4., 4.], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1308, 0.1106, 0.1113, 0.0831, 0.1228, 0.1229, 0.0762, 0.0569, 0.0602,\n",
      "         0.1253],\n",
      "        [0.0909, 0.0893, 0.1677, 0.0562, 0.1352, 0.1536, 0.0561, 0.0709, 0.0628,\n",
      "         0.1173],\n",
      "        [0.0799, 0.0681, 0.1347, 0.0490, 0.2014, 0.0863, 0.0562, 0.0624, 0.1348,\n",
      "         0.1272],\n",
      "        [0.0754, 0.0946, 0.1109, 0.0913, 0.1357, 0.0958, 0.0855, 0.0810, 0.1075,\n",
      "         0.1224]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = model(dummy_input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 計算 CrossEntropy Loss\n",
    "* 請注意哪一個 Loss最適合：我們已經使用 softmax\n",
    "* 因為我們有使用dropout，並隨機產生dummy_input，所以各為學員得到的值會與解答不同，然而步驟原理需要相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import NLLLoss, LogSoftmax, CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8874, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss = criterion(torch.log(output), target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 完成back propagation並更新梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight : Parameter containing:\n",
      "tensor([[ 0.0561, -0.0243, -0.0138,  ...,  0.0052,  0.0050, -0.0564],\n",
      "        [-0.0596, -0.0209, -0.0368,  ..., -0.0110, -0.0238,  0.0553],\n",
      "        [-0.0248,  0.0382,  0.0375,  ...,  0.0294,  0.0419, -0.0398],\n",
      "        ...,\n",
      "        [ 0.0136, -0.0296,  0.0268,  ...,  0.0523,  0.0272,  0.0394],\n",
      "        [ 0.0623, -0.0134, -0.0263,  ..., -0.0568, -0.0504, -0.0492],\n",
      "        [-0.0460,  0.0328,  0.0032,  ...,  0.0151,  0.0242,  0.0274]],\n",
      "       requires_grad=True)\n",
      "\n",
      "\n",
      "grad : tensor([[ -1.0324, -11.3790,   5.2907,  ...,   7.2664,  -3.2597,  11.6453],\n",
      "        [  1.3716,  -1.2888,  -0.0212,  ...,   0.9908,  -0.3720,  -0.7623],\n",
      "        [  0.5514,  -0.3592,   0.0582,  ...,   0.0372,  -0.0315,  -0.1132],\n",
      "        ...,\n",
      "        [ -0.8486,   0.7287,  -0.3879,  ...,   0.0782,   0.3995,  -0.6859],\n",
      "        [  0.5797,  -0.6876,  -0.1785,  ...,   0.7804,   0.0864,  -0.9109],\n",
      "        [  0.6162,   0.3066,  -0.1558,  ...,  -0.3587,  -0.2968,  -0.3819]])\n"
     ]
    }
   ],
   "source": [
    "print('weight : {}'.format(model.layer1.linear[0].weight))\n",
    "print('\\n')\n",
    "print('grad : {}'.format(model.layer1.linear[0].weight.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight : Parameter containing:\n",
      "tensor([[ 0.0571, -0.0233, -0.0148,  ...,  0.0042,  0.0060, -0.0574],\n",
      "        [-0.0606, -0.0199, -0.0358,  ..., -0.0120, -0.0228,  0.0563],\n",
      "        [-0.0258,  0.0392,  0.0365,  ...,  0.0284,  0.0429, -0.0388],\n",
      "        ...,\n",
      "        [ 0.0146, -0.0306,  0.0278,  ...,  0.0513,  0.0262,  0.0404],\n",
      "        [ 0.0613, -0.0124, -0.0253,  ..., -0.0578, -0.0514, -0.0482],\n",
      "        [-0.0470,  0.0318,  0.0042,  ...,  0.0161,  0.0252,  0.0284]],\n",
      "       requires_grad=True)\n",
      "\n",
      "\n",
      "grad : tensor([[ -1.0324, -11.3790,   5.2907,  ...,   7.2664,  -3.2597,  11.6453],\n",
      "        [  1.3716,  -1.2888,  -0.0212,  ...,   0.9908,  -0.3720,  -0.7623],\n",
      "        [  0.5514,  -0.3592,   0.0582,  ...,   0.0372,  -0.0315,  -0.1132],\n",
      "        ...,\n",
      "        [ -0.8486,   0.7287,  -0.3879,  ...,   0.0782,   0.3995,  -0.6859],\n",
      "        [  0.5797,  -0.6876,  -0.1785,  ...,   0.7804,   0.0864,  -0.9109],\n",
      "        [  0.6162,   0.3066,  -0.1558,  ...,  -0.3587,  -0.2968,  -0.3819]])\n"
     ]
    }
   ],
   "source": [
    "print('weight : {}'.format(model.layer1.linear[0].weight))\n",
    "print('\\n')\n",
    "print('grad : {}'.format(model.layer1.linear[0].weight.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 清空 gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight : Parameter containing:\n",
      "tensor([[ 0.0571, -0.0233, -0.0148,  ...,  0.0042,  0.0060, -0.0574],\n",
      "        [-0.0606, -0.0199, -0.0358,  ..., -0.0120, -0.0228,  0.0563],\n",
      "        [-0.0258,  0.0392,  0.0365,  ...,  0.0284,  0.0429, -0.0388],\n",
      "        ...,\n",
      "        [ 0.0146, -0.0306,  0.0278,  ...,  0.0513,  0.0262,  0.0404],\n",
      "        [ 0.0613, -0.0124, -0.0253,  ..., -0.0578, -0.0514, -0.0482],\n",
      "        [-0.0470,  0.0318,  0.0042,  ...,  0.0161,  0.0252,  0.0284]],\n",
      "       requires_grad=True)\n",
      "\n",
      "\n",
      "grad : tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print('weight : {}'.format(model.layer1.linear[0].weight))\n",
    "print('\\n')\n",
    "print('grad : {}'.format(model.layer1.linear[0].weight.grad))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
